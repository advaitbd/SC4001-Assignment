{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c5cb62ac-8e88-43e6-bce9-da20fabf38ff",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c7e82aadc4d77a8b23f7f880449f9e3",
     "grade": false,
     "grade_id": "a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Question A2 (10 marks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26b4ac2a-d56e-4151-8e0a-4a833cbc643e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "26b4ac2a-d56e-4151-8e0a-4a833cbc643e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb28aa752ce5540f5b18d10694b52ea9",
     "grade": false,
     "grade_id": "a22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### In this question, we will determine the optimal batch size for mini-batch gradient descent. Find the optimal batch size for mini-batch gradient descent by training the neural network and evaluating the performances for different batch sizes. Note: Use 5-fold cross-validation on training partition to perform hyperparameter selection. You will have to reconsider the scaling of the dataset during the 5-fold cross validation.\n",
    "\n",
    "* note: some cells are non-editable and cannot be filled, but leave them untouched. Fill up only cells which are provided."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fb9411ad-2324-400e-852e-ff5c0ca716f0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aceec82011f43733c0551ca196f1b16c",
     "grade": false,
     "grade_id": "a2_1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Plot mean cross-validation accuracies on the final epoch for different batch sizes as a scatter plot. Limit search space to batch sizes {128, 256, 512, 1024}. Next, create a table of time taken to train the network on the last epoch against different batch sizes. Finally, select the optimal batch size and state a reason for your selection.\n",
    "\n",
    "This might take a while to run, so plan your time carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b0edc610-21e6-4cc7-9603-59318b961990",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b0edc610-21e6-4cc7-9603-59318b961990",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "909acb3c7ff3883eb5381eb586615d3b",
     "grade": false,
     "grade_id": "libraries",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8e12861-4713-4914-9f4b-8a7381708243",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e8e12861-4713-4914-9f4b-8a7381708243",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed97d9f30da032a5e349047c614efec1",
     "grade": false,
     "grade_id": "a2_1_2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "2. To reduce repeated code, place your\n",
    "\n",
    "- network (MLP defined in QA1)\n",
    "- torch datasets (CustomDataset defined in QA1)\n",
    "- loss function (loss_fn defined in QA1)\n",
    "\n",
    "in a separate file called **common_utils.py**\n",
    "\n",
    "Import them into this file. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked.\n",
    "\n",
    "The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "37a1a982-de85-46de-b890-3b81f79f5887",
   "metadata": {
    "deletable": false,
    "id": "37a1a982-de85-46de-b890-3b81f79f5887",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9db3ca972642b1447dba3ebd5f2db24b",
     "grade": false,
     "grade_id": "import",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from common_utils import MLP, split_dataset, preprocess_dataset, CustomDataset, loss_fn\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess(df):\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = split_dataset(df,['filename','label'],0.3,0)\n",
    "    X_train_scaled, X_test_scaled = preprocess_dataset(X_train, X_test)\n",
    "\n",
    "    return X_train_scaled, y_train, X_test_scaled, y_test\n",
    "\n",
    "df = pd.read_csv('simplified.csv')\n",
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "\n",
    "X_train_scaled, y_train, X_test_scaled, y_test = preprocess(df)\n",
    "\n",
    "train_data = CustomDataset(X_train_scaled, y_train)\n",
    "test_data = CustomDataset(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa562e7-23c3-4920-ae63-4563bf30e39d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5aa562e7-23c3-4920-ae63-4563bf30e39d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae6b33318200b4bc38d431576963edb1",
     "grade": true,
     "grade_id": "correct_import",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82ea67d6-1eb4-428d-9407-9d988e927ff6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "82ea67d6-1eb4-428d-9407-9d988e927ff6",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c738d3b4888de90dda8c532036bc5fe5",
     "grade": false,
     "grade_id": "a2_1_3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "3. Define different folds for different batch sizes to get a dictionary of training and validation datasets. Preprocess your datasets accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "deab683a-2c9e-4e62-823a-e8b4a186bda8",
   "metadata": {
    "deletable": false,
    "id": "deab683a-2c9e-4e62-823a-e8b4a186bda8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d02dac62baa528c191eb4f47b2495406",
     "grade": false,
     "grade_id": "dataset",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_cv_folds_for_batch_sizes(parameters, X_train, y_train):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    X_train_scaled_dict(dict) where X_train_scaled_dict[batch_size] is a list of the preprocessed training matrix for the different folds.\n",
    "    X_val_scaled_dict(dict) where X_val_scaled_dict[batch_size] is a list of the processed validation matrix for the different folds.\n",
    "    y_train_dict(dict) where y_train_dict[batch_size] is a list of labels for the different folds\n",
    "    y_val_dict(dict) where y_val_dict[batch_size] is a list of labels for the different folds\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train_scaled_dict = {}\n",
    "    X_val_scaled_dict = {}\n",
    "    y_train_dict = {}\n",
    "    y_val_dict = {}\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "    for batch_size in parameters:\n",
    "        X_train_scaled_dict[batch_size] = []\n",
    "        X_val_scaled_dict[batch_size] = []\n",
    "        y_train_dict[batch_size] = []\n",
    "        y_val_dict[batch_size] = []\n",
    "\n",
    "        \n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            X_train_scaled_dict[batch_size].append(X_train_fold)\n",
    "            X_val_scaled_dict[batch_size].append(X_val_fold)\n",
    "            y_train_dict[batch_size].append(y_train_fold)\n",
    "            y_val_dict[batch_size].append(y_val_fold)\n",
    "\n",
    "    return X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict\n",
    "\n",
    "batch_sizes = [128, 256, 512, 1024]\n",
    "X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict = generate_cv_folds_for_batch_sizes(batch_sizes, X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ca332-9676-42bd-9801-0f5f4157a777",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "235ca332-9676-42bd-9801-0f5f4157a777",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ae5f281cd84f4d36f81f2ae126cf915",
     "grade": true,
     "grade_id": "correct_dataset",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df744af-f485-4871-9e0a-70fd41d1df4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8df744af-f485-4871-9e0a-70fd41d1df4d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dcf6be1ad49306172e6f27243e613f2",
     "grade": true,
     "grade_id": "correct_dataset2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "558aa470-6d7e-454c-9cda-9ad881d58c53",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "064d68c9708b5e3f1e2463001b6d78b4",
     "grade": false,
     "grade_id": "a2_1_4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "4. Perform hyperparameter tuning for the different batch sizes with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3107ebe9-d121-4510-9782-2a62d32258d0",
   "metadata": {
    "deletable": false,
    "id": "3107ebe9-d121-4510-9782-2a62d32258d0",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9665887943f38ae7bed6c1d8351903b",
     "grade": true,
     "grade_id": "hyperparameter_tuning",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for batch_size = 128\n",
      "Fold 1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.700155  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.688382 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.675743 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.686896  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.674810 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.660796 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.647043  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.659785 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.648022 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.637716  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.645977 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.637697 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.623387  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.630230 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.639161 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.570981  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.612567 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.627717 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.569203  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.594788 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.619381 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.631187  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.578815 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.599611 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.525549  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.563618 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.601039 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.488760  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.546358 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.583538 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.544745  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.537984 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.591568 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.552714  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.512024 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.578779 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.470435  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.510082 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.592685 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.554320  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.490348 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.569038 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.507715  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.474026 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.559653 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.407913  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.471964 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.559841 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.475874  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.459061 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.548575 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.437968  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.443289 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.559623 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.435685  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.425674 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.543507 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.451050  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.424671 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.544728 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.351413  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.415378 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.534862 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.371528  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.397398 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.557866 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.377017  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.391316 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.532058 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.391608  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.382645 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.536784 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.417697  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.376121 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.540474 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.401414  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.362652 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.540696 \n",
      "\n",
      "Done!\n",
      "Fold 2\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.698713  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.5%, Avg loss: 0.687546 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.684208 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.698605  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.672791 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.672933 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.705088  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.657587 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 0.678540 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.637287  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.644367 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.659307 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.639456  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.627335 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.644328 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.623804  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.611629 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.662895 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.714797  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.598915 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.630283 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.608393  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.587013 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.624825 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.544748  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.577277 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.618430 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.621451  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.551579 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.615489 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.612156  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.545938 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.591363 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.521251  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.526724 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.600300 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.517057  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.517271 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.572739 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.523217  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.502189 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.587938 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.522878  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.492306 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.576167 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.432899  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.479457 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.560197 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.457184  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.466866 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.566274 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.462855  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.457302 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.558332 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.468774  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.449547 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.568488 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.428243  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.429512 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.551998 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.430898  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.430392 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.534675 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.421152  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.416122 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.553755 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.408010  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.408854 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.536786 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.403330  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.398220 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.552674 \n",
      "\n",
      "Done!\n",
      "Fold 3\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.699323  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.3%, Avg loss: 0.686439 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 0.678625 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.660630  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.670336 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.662867 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.664830  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.658844 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.655414 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.672916  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.643939 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.662723 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.612759  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.631225 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.646857 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.592779  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.620359 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.642569 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.540980  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.602960 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.641112 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.583091  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.588809 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.622555 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.591968  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.573399 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.614097 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.590193  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.559460 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.598833 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.543923  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.542782 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.599980 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.520889  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.530862 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.573729 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.518546  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.514503 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.582032 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.539145  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.492287 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.576482 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.469294  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.490874 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.572219 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.432744  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.480559 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.573017 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.462368  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.470281 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.545939 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.381470  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.454870 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.565964 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.330588  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.450801 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.539602 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.300073  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.419530 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.556569 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.333634  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.424396 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.560400 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.455159  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.415168 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.547463 \n",
      "\n",
      "Done!\n",
      "Fold 4\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.691243  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.685953 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 0.682373 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.686703  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.671885 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.680099 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.646739  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.659201 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.664416 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.683571  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.639316 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.648049 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.598628  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.627828 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.653494 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.643119  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.611792 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.644913 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.608440  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.596443 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.624666 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.547093  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.569297 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.621929 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.570750  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.555695 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.601755 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.544040  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.548873 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.599655 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.526366  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.530126 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.592124 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.526248  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.523193 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.589612 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.543223  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.501797 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.563436 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.504558  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.495023 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.577625 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.491308  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.474147 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.572655 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.503201  [  128/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.468264 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.569843 \n",
      "\n",
      "Done!\n",
      "Fold 5\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.692315  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 54.7%, Avg loss: 0.686920 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.675025 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.688732  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.668533 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.663280 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.616902  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.655453 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.658209 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.607210  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.642156 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.644639 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.611335  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.623566 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.630650 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.564696  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.616711 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.632469 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.617389  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.601695 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.604604 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.570313  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.583453 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.587008 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.580052  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.573716 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.609351 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.614729  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.558415 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.585360 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.529565  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.535838 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.563625 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.528707  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.520215 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.579687 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.471224  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.506013 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.561850 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.447779  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.495706 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.562722 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.509361  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.483375 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.572860 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.454634  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.475140 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.552912 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.460277  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.459802 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.542941 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.427907  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.454759 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.538635 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.439563  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.434050 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.536495 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.476043  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.432616 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.543582 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.373486  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.422604 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.535269 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.405002  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.412068 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.513601 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.419527  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.397439 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.527618 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.343498  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.394493 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.533995 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.345676  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.393366 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.509742 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.321199  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.373967 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.517496 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.298853  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.363024 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.516996 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.362283  [  128/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.363518 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.526796 \n",
      "\n",
      "Done!\n",
      "Training for batch_size = 256\n",
      "Fold 1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.693111  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.688668 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.679944 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.689596  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.679124 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.666366 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.679814  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.669595 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.659200 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.662340  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.656695 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.648548 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.650273  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.643175 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.648013 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.590728  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.632790 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.632394 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.622192  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.616698 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.631798 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.614636  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.610863 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.621291 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.560444  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.592441 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.616301 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.580293  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.579150 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.609229 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.567738  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.567203 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.608393 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.569676  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.553724 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.598733 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.531070  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.543204 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.603664 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.530653  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.531444 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.593462 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.525967  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.522996 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.587916 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.534805  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.500650 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.585176 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.455002  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.502817 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.576287 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.466524  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.494480 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.567989 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.446582  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.475033 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.574565 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.512750  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.462146 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.566388 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.468433  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.452446 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.559167 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.459390  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.447128 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.544870 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.423520  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.440793 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.567832 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.431913  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.435027 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.563051 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.416534  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.426311 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.562081 \n",
      "\n",
      "Done!\n",
      "Fold 2\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.690600  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.7%, Avg loss: 0.687181 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 0.684617 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.674662  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.674094 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.675443 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.666823  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.664070 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.671936 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.656156  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.653688 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.675605 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.662359  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.640650 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.667320 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.617469  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.631843 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.651036 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.635904  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.621463 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.640194 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.614795  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.607298 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.641190 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.584315  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.592747 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.630905 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.581243  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.582320 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.619749 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.579661  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.577648 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.612425 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.552674  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.559394 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.610435 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.518830  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.549171 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.595161 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.501853  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.540488 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.591106 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.510637  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.527267 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.586407 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.503788  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.515144 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.578986 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.467294  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.493701 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.567331 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.448481  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.490096 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.572280 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.481571  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.478935 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.562260 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.477889  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.468316 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.558056 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.479473  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.460992 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.558686 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.397159  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.457112 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.547168 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.388502  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.445292 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.540846 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.401866  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.433102 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.545472 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.387058  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.417495 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.532404 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.433407  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.408528 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.548760 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.450224  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.404690 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.537426 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.396344  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.403061 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.540863 \n",
      "\n",
      "Done!\n",
      "Fold 3\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.693747  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.689088 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 0.682748 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.688059  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.678075 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.680128 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.664248  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.664053 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.667464 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.659409  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.649616 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.661930 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.640178  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.637502 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.657051 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.625735  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.627506 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.647724 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.630480  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.610791 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.634330 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.556862  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.595184 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.623965 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.555805  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.589022 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.625496 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.563431  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.577170 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.614590 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.590087  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.564108 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.612090 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.571441  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.551350 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.599686 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.558472  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.538015 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.595526 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.466995  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.522122 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.582300 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.522971  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.504336 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.569775 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.489928  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.500391 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.560940 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.521529  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.496294 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.553972 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.391692  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.482172 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.558383 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.446726  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.466651 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.542377 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.444073  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.448680 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.542877 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.464025  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.446352 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.545630 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.389581  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.438427 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.527239 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.354414  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.431779 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.534289 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.378584  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.412800 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.527946 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.397648  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.412179 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.544723 \n",
      "\n",
      "Done!\n",
      "Fold 4\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.696417  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.690078 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.687654 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.681890  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.678427 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.682878 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.672261  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.665666 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 0.673270 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.640895  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.654916 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.667409 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.660196  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.641750 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.659462 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.613369  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.629751 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.644514 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.605437  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.614249 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.644492 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.632880  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.603476 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.626984 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.601775  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.587695 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.619907 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.546958  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.569878 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.611042 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.529459  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.556828 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.603228 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.534786  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.542694 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.611430 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.586815  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.533298 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.604528 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.558177  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.516983 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.585233 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.495461  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.503461 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.562204 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.455341  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.483930 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.586412 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.515512  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.478631 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.550986 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.471481  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.462217 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.552437 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.419885  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.454487 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.557360 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.405523  [  256/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.455502 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.553784 \n",
      "\n",
      "Done!\n",
      "Fold 5\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.692067  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.688921 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Avg loss: 0.681008 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.692156  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.675685 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.670184 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.672188  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 59.8%, Avg loss: 0.665677 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.662456 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.631977  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.654014 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.653522 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.648557  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.643865 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.642554 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.612743  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.631964 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.638311 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.613095  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.620712 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.625081 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.583740  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.602709 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.618862 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.590581  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.593591 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.620449 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.565390  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.573750 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.600688 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.589363  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.561346 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.589222 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.563162  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.551968 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.576418 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.541404  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.546316 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.576463 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.546745  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.532338 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.575737 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.545535  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.521225 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.571549 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.507810  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.502039 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.552130 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.479575  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.497707 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.542647 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.503559  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.487053 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.545614 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.468414  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.475739 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.546531 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.430284  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.460958 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.538193 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.447888  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.457610 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.529573 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.445714  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.437815 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.533624 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.438372  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.440143 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.523944 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.408362  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.419233 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.524899 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.411020  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.413620 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.521647 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.420632  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.404527 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.509621 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.374295  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.401457 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.505005 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.387663  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.394188 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.509143 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.376366  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.384081 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.508053 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.319996  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.381568 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.504175 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.369147  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.367683 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.502569 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.381456  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.361049 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.513342 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.323301  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.357440 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.497669 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.318197  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.358528 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.509522 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.280011  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.336083 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.503012 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.364905  [  256/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.347655 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.509580 \n",
      "\n",
      "Done!\n",
      "Training for batch_size = 512\n",
      "Fold 1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.693491  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.690823 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.684548 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.686160  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 55.7%, Avg loss: 0.683961 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.672645 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.666203  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.676454 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.667037 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.663134  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.666850 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.662809 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.646237  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.653198 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.648259 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.635290  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.646094 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.651460 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.671672  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.639517 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.637218 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.630450  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.626881 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.632371 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.628057  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.614776 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.628833 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.602207  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.614137 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.627704 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.614223  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.599305 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.618516 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.599837  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.596110 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.618893 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.544519  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.575874 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.617502 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.531888  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.568537 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.603058 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.521501  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.554146 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.594430 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.525398  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.550096 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.604240 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.543300  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.545976 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.585119 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.548752  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.529908 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.595225 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.545608  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.532894 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.574872 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.535332  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.510600 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.571991 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.485097  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.506799 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.568894 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.514287  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.497562 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.565467 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.484433  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.484158 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.563672 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.456401  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.491642 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.561360 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.459200  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.472335 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.567140 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.506818  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.468392 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.566304 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.476583  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.467163 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.563005 \n",
      "\n",
      "Done!\n",
      "Fold 2\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.696408  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 53.5%, Avg loss: 0.690607 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 0.689107 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.683270  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 56.6%, Avg loss: 0.681317 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.684376 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.669635  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.672504 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.676562 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.663506  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.662897 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.671466 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.643199  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.653278 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Avg loss: 0.670282 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.636911  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.643585 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.669437 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.641695  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.637184 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.662472 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.598880  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.627137 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 0.646766 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.622470  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.622162 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.663738 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.624889  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.611271 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.632898 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.586408  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.602184 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.640217 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.601940  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.591891 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.622647 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.589923  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.588188 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.623385 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.571204  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.574696 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.624579 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.555632  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.0%, Avg loss: 0.570186 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.619666 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.540712  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.560209 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.608522 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.584320  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.552340 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.599516 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.560851  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.542919 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.589301 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.509090  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.527613 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.601496 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.496071  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.525299 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.580872 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.510441  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.511104 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.587338 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.508901  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.506391 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.569656 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.516596  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.504773 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.566691 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.504496  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.496945 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.554641 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.488858  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.484540 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.566513 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.462736  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.477544 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.569883 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.462872  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.467188 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.548086 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.429838  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.469591 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.554083 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.468769  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.457929 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.547545 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.386419  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.443410 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.540537 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.434204  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.437957 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.563563 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.438045  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.437244 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.549493 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.405594  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.426004 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.526798 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.412758  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.414861 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.527622 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.389074  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.420614 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.527893 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.411299  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.405471 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.530814 \n",
      "\n",
      "Done!\n",
      "Fold 3\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.695464  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 53.0%, Avg loss: 0.690453 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.686733 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.685785  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.682582 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.677917 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.678712  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.674161 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.682280 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.663693  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.666562 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.675095 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.650393  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.658575 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Avg loss: 0.669948 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.651311  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.646475 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.669190 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.625717  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.644108 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.658212 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.623737  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.635290 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.659937 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.641051  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.623360 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.648273 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.624919  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.624218 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.648794 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.605227  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.606745 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.641907 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.601926  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.600023 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.650113 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.579102  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.589948 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.641985 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.555247  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.581453 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.632234 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.546788  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.573946 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.623296 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.574791  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.563258 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.613423 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.550334  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.552922 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.610624 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.538975  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.548590 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.612997 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.527035  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.539928 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.590732 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.521540  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.532638 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.594567 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.538724  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.524624 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.587557 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.514292  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.516662 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.581100 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.488296  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.505295 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.590375 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.502103  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.489898 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.573333 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.538713  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.490268 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.580766 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.468993  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.488109 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.569299 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.425567  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.475679 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.573761 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.422215  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.473310 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.575948 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.425117  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.469730 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.567556 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.451542  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.449697 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.562368 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.471185  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.452191 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.553957 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.379031  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.435552 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.537054 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.427784  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.438006 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.553268 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.406012  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.429286 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.551133 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.417950  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.433052 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.541646 \n",
      "\n",
      "Done!\n",
      "Fold 4\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.692000  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.3%, Avg loss: 0.689765 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.684538 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.690664  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.679839 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.685460 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.686212  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.673080 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.675191 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.670336  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.664784 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.672120 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.657492  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.660096 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.665469 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.652271  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.649191 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.663605 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.623990  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.639549 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.657530 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.635838  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.626725 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.647918 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.614826  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.618495 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.646514 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.620373  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.619763 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.644788 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.598469  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.605209 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.620473 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.604940  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.594443 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.634263 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.586751  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.588479 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.626987 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.561085  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.571534 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.620431 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.588878  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.571986 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.608985 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.557270  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.548904 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.598864 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.570378  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.547629 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.598823 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.534702  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.541439 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.594774 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.498686  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.525417 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.576716 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.525232  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.520618 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.589196 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.555261  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.513868 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.584340 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.499991  [  512/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.500738 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.592865 \n",
      "\n",
      "Done!\n",
      "Fold 5\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.694935  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 53.5%, Avg loss: 0.690354 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.684130 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.686021  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.684524 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 0.674667 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.666169  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.672968 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.673653 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.657760  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.659920 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.665103 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.674768  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.654611 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.657506 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.648367  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.650213 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.657558 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.641795  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.644659 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.649963 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.628284  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.635968 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.645654 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.633351  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.617527 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 0.643006 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.612479  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.617179 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.645520 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.606130  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.604661 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.632553 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.573715  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.592908 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.618918 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.595259  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.591626 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.612169 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.581915  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.576712 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.609512 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.572313  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.565261 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.615256 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.545523  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.553342 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.595332 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.536335  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.548928 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.592686 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.521734  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.543292 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.587114 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.523483  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.523742 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.574956 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.536223  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.533570 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.583198 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.509981  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.515099 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.583932 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.496916  [  512/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.520020 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.579676 \n",
      "\n",
      "Done!\n",
      "Training for batch_size = 1024\n",
      "Fold 1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.693755  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 52.2%, Avg loss: 0.691407 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.687555 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.686094  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.5%, Avg loss: 0.687263 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.681338 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.680559  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.681581 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.676348 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.677037  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.676864 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.668712 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.680520  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.670121 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.662048 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.655071  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.661266 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.653921 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.663415  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.654125 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.649736 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.635478  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.648941 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.646267 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.629624  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.639088 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.638002 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.632725  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.634240 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.643028 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.626000  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.633972 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631434 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.617001  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.624443 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.628445 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.622126  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.616588 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.632796 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.607324  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.600372 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.621537 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.600419  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.598626 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.620750 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.590801  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.591355 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.616006 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.581987  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.583567 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.613290 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.594620  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.573015 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.610483 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.576326  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.568958 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.610869 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.548726  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.566841 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.608933 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.565171  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.549384 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.604838 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.547175  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.544899 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.597181 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.568399  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.536340 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.602897 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.533687  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.538707 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.593048 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.497820  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.536314 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.587364 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.533872  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.524780 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 0.581033 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.517272  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.520379 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.592533 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.530398  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.508972 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.580897 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.476203  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.501836 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.573975 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.492774  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.501021 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.576564 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.476670  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.488384 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.579320 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.485179  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.477554 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.571990 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.489117  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.7%, Avg loss: 0.480686 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.569863 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.463178  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.474655 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.566507 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.467397  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.464082 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.579607 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.470887  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.472915 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.555507 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.452767  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.457454 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.560740 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.467093  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.459042 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.551596 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.433530  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.450055 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.552222 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.449298  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.440206 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.562535 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.438878  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.431755 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.548197 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.421716  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.429282 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.546657 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.414394  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.432694 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.557995 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.411697  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.423587 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.549658 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.434810  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.420226 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.546162 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.403301  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.405069 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.551048 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.424275  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.413163 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.555500 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.416520  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.404128 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.538663 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.417959  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.399071 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.539352 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.385319  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.398179 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.540082 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.399938  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.395779 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.542401 \n",
      "\n",
      "Done!\n",
      "Fold 2\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.695846  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 51.7%, Avg loss: 0.691637 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.690705 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.685934  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 55.1%, Avg loss: 0.686154 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 0.686968 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.681707  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.680809 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 0.683573 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.675025  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.673123 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.677083 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.665747  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.667961 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.673846 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.672652  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.661354 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.670534 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.650339  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.654432 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.667993 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.655574  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.646407 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.662190 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.642537  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.636905 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.661866 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.638869  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.635842 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.657563 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.617891  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.622837 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.651781 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.634297  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.617039 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.647682 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.633952  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.615990 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.640762 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.601369  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.606961 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.640484 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.579601  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.596523 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.635914 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.589232  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.587978 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.632084 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.584797  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.584123 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.620234 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.552846  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.576256 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.612895 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.558524  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.569518 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.606211 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.560812  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.563296 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.604375 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.556941  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.551643 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.604368 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.533203  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.543129 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.599693 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.535717  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.538991 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.594842 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.534144  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.538097 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.596882 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.538111  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.531082 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.583261 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.551341  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.521689 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.585145 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.504679  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.508821 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.575447 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.517833  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.499514 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.565690 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.472638  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.497134 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.568723 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.501548  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.490436 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.563547 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.485275  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.484424 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.557774 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.478319  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.478604 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.569167 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.509300  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.476198 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.553543 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.455471  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.463305 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.556121 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.439836  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.460887 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.545782 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.440295  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.458384 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.545368 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.435611  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.449161 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.552806 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.463764  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.444315 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.544122 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.475491  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.442145 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.545492 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.432794  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.437429 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.543078 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.425570  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.432556 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.532062 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.410329  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.421468 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.536894 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.404575  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.419972 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.533599 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.415748  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.413553 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.540314 \n",
      "\n",
      "Done!\n",
      "Fold 3\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.694277  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 51.7%, Avg loss: 0.691898 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.7%, Avg loss: 0.688814 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.686322  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 54.5%, Avg loss: 0.686430 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.684478 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.686769  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.2%, Avg loss: 0.680365 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.680393 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.678471  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.674367 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.679637 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.670788  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 58.7%, Avg loss: 0.668340 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.677906 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.668277  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.659889 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.669233 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.650863  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 61.8%, Avg loss: 0.653651 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.667173 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.651155  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.640542 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 0.663419 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.649748  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.635164 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.654794 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.626426  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.627697 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.657727 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.625906  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.622475 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.644126 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.618102  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.609500 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 0.641731 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.601380  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.601774 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.639573 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.608370  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.596328 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.634552 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.590629  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.595894 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630069 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.573087  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.579645 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.614481 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.555904  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.572142 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.620576 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.591969  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.573645 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.614549 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.565522  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.2%, Avg loss: 0.563310 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.604983 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.543522  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.553395 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.602666 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.550132  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.542514 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.601279 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.519575  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.540326 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.586920 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.544442  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.531862 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.596605 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.528353  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.523107 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.580640 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.501033  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.510561 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.581388 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.524133  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.503112 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.573486 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.501649  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.500030 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.574544 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.515680  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.498274 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.568836 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.480215  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.487637 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.565053 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.492636  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.487188 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.561817 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.487348  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.476138 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.556519 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.475180  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.470086 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.551524 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.467245  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.454435 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.549983 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.432727  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.455341 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.3%, Avg loss: 0.558029 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.455790  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.452721 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.545956 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.462358  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.438536 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.551159 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.448984  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.445953 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.552107 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.420366  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.434078 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.544120 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.438608  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.410262 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.542258 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.429035  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.421882 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.541947 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.398456  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.422346 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.538288 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.414675  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.411855 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.534459 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.410234  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.403866 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.523199 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.402751  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.407375 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.531923 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.396426  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.406351 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.549458 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.387964  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.393207 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.525182 \n",
      "\n",
      "Done!\n",
      "Fold 4\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.692193  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 52.0%, Avg loss: 0.690853 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 0.688273 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.688969  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 56.6%, Avg loss: 0.685260 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.683452 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.677506  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 57.3%, Avg loss: 0.679248 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.682076 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.669448  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.670445 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 0.677585 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.661975  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.665834 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.676030 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.667827  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.655380 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 0.674626 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.672545  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.650467 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.673994 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.653653  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 62.0%, Avg loss: 0.644493 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.661332 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.647570  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.636615 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.658795 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.625280  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.628066 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.657456 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.630495  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.622241 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.648949 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.616620  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.613872 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.643232 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.615901  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.602689 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.636579 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.603680  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.597009 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.635227 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.598292  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.594862 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 0.636933 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.597885  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.586014 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.629850 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.590337  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.577140 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.627239 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.570725  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.569140 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.622757 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.557944  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.557787 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.609043 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.557835  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.548500 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.616403 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.526469  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.544839 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.600844 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.544203  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.541182 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.606539 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.534766  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.528619 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.607334 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.530196  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.522932 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.592534 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.516026  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.515871 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.595235 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.509370  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.510087 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.596447 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.508976  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.499599 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.589306 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.498833  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.495412 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.583795 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.507530  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.488023 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.579961 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.491889  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.480173 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.585375 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.470848  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.481451 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.583739 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.466193  [ 1024/ 6751]\n",
      "Train Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.475003 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.585983 \n",
      "\n",
      "Done!\n",
      "Fold 5\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.693550  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.690425 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.686282 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.687349  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 56.5%, Avg loss: 0.685005 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.679143 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.677289  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.677469 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.677992 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.671427  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.670825 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 0.672058 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.661933  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.664189 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.667532 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.658261  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.658402 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 0.664315 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.664488  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.650255 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.658142 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.641327  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.642483 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 0.648764 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.634393  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.635919 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.645047 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.633557  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.633007 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.636879 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.634792  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.623186 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.634272 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.610882  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.617839 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.624901 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.616767  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.607535 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.628627 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.605162  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.601942 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.610299 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.600317  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 67.9%, Avg loss: 0.596620 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.608686 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.580729  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.587907 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.601547 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.580696  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 69.1%, Avg loss: 0.579715 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.600087 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.548414  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.572377 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.601811 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.568289  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.569215 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.5%, Avg loss: 0.583741 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.574155  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.556745 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.583474 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.547267  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.543020 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.581897 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.558434  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.541860 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.578268 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.521694  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.529374 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.577168 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.508671  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.521141 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.565484 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.521498  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.519311 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.557401 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.513893  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.510732 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.561002 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.504301  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.501780 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.552836 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.490409  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.500374 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.547848 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.499869  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.491648 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.550418 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.506442  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.485291 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.547900 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.500487  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.475590 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.542717 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.468227  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.471154 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.538107 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.469615  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.462940 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.534272 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.454104  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.459693 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.533816 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.471962  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.457100 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.536463 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.444406  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.455409 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.533779 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.434104  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 78.6%, Avg loss: 0.454343 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.528815 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.415351  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.448450 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.527898 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.445771  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.437339 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.534652 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.435869  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.437809 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.539316 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.430982  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.429773 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.525228 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.410319  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.418608 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.531475 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.408486  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.412116 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.522656 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.406807  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.410437 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.524236 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.434290  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.417771 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.524792 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.419368  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.404213 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.520947 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.397622  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.400565 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.521464 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.377516  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.395358 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.526773 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.378029  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.383912 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.513838 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.400269  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.388841 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.525817 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.362584  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.378810 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.520558 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.386025  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.383045 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.513827 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.382120  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.379656 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.517920 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.404133  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.369500 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.520064 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.354615  [ 1024/ 6752]\n",
      "Train Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.364804 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.519596 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from common_utils import EarlyStopper\n",
    "\n",
    "epochs = 100\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=0)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "        correct += ((pred > 0.5).type(torch.float) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Train Error: \\n Accuracy: {(correct*100):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "    \n",
    "    return train_loss, correct\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += ((pred > 0.5).type(torch.float) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_loss, correct\n",
    "\n",
    "def find_optimal_hyperparameter(X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict, parameters, parameter_name):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    cross_validation_accuracies(list) where cross_validation_accuracies[i] is the cross validation accuracy for the ith parameter value\n",
    "    cross_validation_times(list) where cross_validation_times[i] is the mean time taken on the last epoch for the ith parameter value\n",
    "    \"\"\"\n",
    "\n",
    "    cross_validation_accuracies = []\n",
    "    cross_validation_times = []\n",
    "\n",
    "    for parameter in parameters:\n",
    "        print(f\"Training for {parameter_name} = {parameter}\")\n",
    "        accuracies = []\n",
    "        times = []\n",
    "\n",
    "        for i in range(5):\n",
    "            print(f\"Fold {i+1}\")\n",
    "            model = MLP(77,128,1)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "            loss_fn = nn.BCELoss()\n",
    "            early_stopper = EarlyStopper(patience=3, min_delta=0)\n",
    "\n",
    "            train_data = CustomDataset(X_train_scaled_dict[parameter][i], y_train_dict[parameter][i])\n",
    "            val_data = CustomDataset(X_val_scaled_dict[parameter][i], y_val_dict[parameter][i])\n",
    "\n",
    "            train_dataloader = DataLoader(train_data, batch_size=parameter, shuffle=True)\n",
    "            val_dataloader = DataLoader(val_data, batch_size=parameter, shuffle=True)\n",
    "\n",
    "            train_loss, test_loss = [], []\n",
    "            train_acc, test_acc = [], []\n",
    "            epoch_times = []\n",
    "            \n",
    "            for t in range(epochs):\n",
    "                \n",
    "                print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "                # Training Loop\n",
    "                start = time.time()\n",
    "                tr_loss, acc = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "                train_loss.append(tr_loss), train_acc.append(acc)\n",
    "                end = time.time()\n",
    "                epoch_times.append(end-start)\n",
    "\n",
    "                # Validation Loop\n",
    "                te_loss, acc = test_loop(val_dataloader, model, loss_fn)\n",
    "                test_loss.append(te_loss), test_acc.append(acc)\n",
    "\n",
    "                if early_stopper.early_stop(te_loss): \n",
    "                    print(\"Done!\")                    \n",
    "                    break\n",
    "            \n",
    "            # Accuracy for the current Fold at last Epoch\n",
    "            accuracies.append(test_acc[-1])\n",
    "\n",
    "            # Time taken for the current Fold at last Epoch\n",
    "            times.append(epoch_times[-1])\n",
    "\n",
    "        # Mean Accuracy for the Batch Size (Mean of Accuracy at Last Epoch for each Fold)\n",
    "        cross_validation_accuracies.append(np.mean(accuracies))\n",
    "\n",
    "        # Mean Time taken for the Batch Size (Mean of Time at Last Epoch for each Fold)\n",
    "        cross_validation_times.append(np.mean(times))\n",
    "\n",
    "    return cross_validation_accuracies, cross_validation_times\n",
    "\n",
    "batch_sizes = [128, 256, 512, 1024]\n",
    "cross_validation_accuracies, cross_validation_times = find_optimal_hyperparameter(X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict, batch_sizes, 'batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64384c9c-ddd5-4460-bf37-b9977443a65c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "64384c9c-ddd5-4460-bf37-b9977443a65c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "975e552e751c4efb2cec0eac214f85cd",
     "grade": true,
     "grade_id": "correct_hyperparameter_tuning",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b6756ab6-92e0-4a5e-b4b9-aebe009f5480",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69421943e22521de848bb03a50f57767",
     "grade": false,
     "grade_id": "a2_1_5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "5. Plot scatterplot of mean cross validation accuracies for the different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
   "metadata": {
    "deletable": false,
    "id": "8fa3afdf-eed6-47b9-9acc-bc2304c46ec3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17599eb29fd6e3a1e2812f0ff7cba983",
     "grade": true,
     "grade_id": "plot",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "def plot_cross_validation_accuracies(cross_validation_accuracies, parameters, parameter_name):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.plot(parameters, cross_validation_accuracies, marker='x',linestyle='None')\n",
    "    plt.xlabel(parameter_name)\n",
    "    plt.ylabel('Cross Validation Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "plot_cross_validation_accuracies(cross_validation_accuracies, batch_sizes, 'batch_size')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "baab6e4d-4e8b-4358-a68d-682f60db4a06",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11e8d298b5774c4044f1c3f950c46214",
     "grade": false,
     "grade_id": "a2_1_6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "6. Create a table of time taken to train the network on the last epoch against different batch sizes. Select the optimal batch size and state a reason for your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
   "metadata": {
    "deletable": false,
    "id": "081aa567-cd92-4749-93fd-fc6608a1f6ae",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c18e30a9850c282ad725336848222a62",
     "grade": false,
     "grade_id": "times",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Last Epoch Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>0.057304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>0.047291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512</td>\n",
       "      <td>0.044142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>0.044243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Batch Size  Last Epoch Time\n",
       "0         128         0.057304\n",
       "1         256         0.047291\n",
       "2         512         0.044142\n",
       "3        1024         0.044243"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Batch Size': batch_sizes,\n",
    "                   'Last Epoch Time': cross_validation_times,             \n",
    "                  })\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "38c040eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Last Epoch Time</th>\n",
       "      <th>Percentage Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>0.057304</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>0.047291</td>\n",
       "      <td>-17.473116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>0.044243</td>\n",
       "      <td>-6.445976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512</td>\n",
       "      <td>0.044142</td>\n",
       "      <td>-0.228488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Batch Size  Last Epoch Time  Percentage Difference\n",
       "0         128         0.057304                    NaN\n",
       "1         256         0.047291             -17.473116\n",
       "3        1024         0.044243              -6.445976\n",
       "2         512         0.044142              -0.228488"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort df by Last Epoch Time and create a column measuring the percentage difference between each\n",
    "\n",
    "df = df.sort_values(by=['Last Epoch Time'], ascending=False)\n",
    "df['Percentage Difference'] = df['Last Epoch Time'].pct_change()*100\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1c83d786-706b-46d2-9220-3b09e4c473b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1c83d786-706b-46d2-9220-3b09e4c473b3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2fc4a52c2a0af7ea586ea85cec9b3e9",
     "grade": true,
     "grade_id": "correct_times",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Cross Validation Accuracy</th>\n",
       "      <th>Percentage Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>0.730064</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>0.729828</td>\n",
       "      <td>-0.032314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>0.729710</td>\n",
       "      <td>-0.016254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512</td>\n",
       "      <td>0.712404</td>\n",
       "      <td>-2.371535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Batch Size  Cross Validation Accuracy  Percentage Difference\n",
       "0         128                   0.730064                    NaN\n",
       "3        1024                   0.729828              -0.032314\n",
       "1         256                   0.729710              -0.016254\n",
       "2         512                   0.712404              -2.371535"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_cv_accuracies = pd.DataFrame({'Batch Size': batch_sizes,\n",
    "                   'Cross Validation Accuracy': cross_validation_accuracies                   \n",
    "                  })\n",
    "\n",
    "batch_cv_accuracies = batch_cv_accuracies.sort_values(by=['Cross Validation Accuracy'], ascending=False)\n",
    "batch_cv_accuracies['Percentage Difference'] = batch_cv_accuracies['Cross Validation Accuracy'].pct_change()*100\n",
    "batch_cv_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d46dfd1c-1d3c-46e4-98d6-21c2672ad31b",
   "metadata": {
    "deletable": false,
    "id": "d46dfd1c-1d3c-46e4-98d6-21c2672ad31b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38690f32ec506325fc73c8353b77d041",
     "grade": false,
     "grade_id": "batch_size",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimal_batch_size = 256\n",
    "reason = \"We take 256 as the optimal batch size as the drop in accuracy from 128 is not significant (<0.05%) while the time taken to train on the last epoch is reduced by more than 20%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ff7b5-6a77-47d4-941e-37bc495b6558",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "096ff7b5-6a77-47d4-941e-37bc495b6558",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f695b961ed43ec6a31b7647e078fd8d6",
     "grade": true,
     "grade_id": "correct_batch_size",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
